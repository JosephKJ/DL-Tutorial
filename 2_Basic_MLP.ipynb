{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "2_Basic_MLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephKJ/DL-Tutorial/blob/master/2_Basic_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaNoYzxOGWcJ",
        "colab_type": "text"
      },
      "source": [
        "# Lets BackProp!\n",
        "\n",
        "We will train an MLP with one hidden layer.\n",
        "\n",
        "This tutorial has two parts: \n",
        "\n",
        "* Implementing Back-propagation from scratch \n",
        "* Using the in-built 'Autograd' module to train the MLP network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZQftHM1GWcL",
        "colab_type": "text"
      },
      "source": [
        "## Import all the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf6rbexvGWcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0hxC_ozGWcP",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FKs9uoJGWcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32 # Batch size\n",
        "input_dim = 784 # Input dimension (For MNIST dataset each image is of size 28 x 28 = 784)\n",
        "num_of_hidden_nodes = 100 # number of hidden nodes in hidden layer\n",
        "output_dim = 10 # Number of output nodes = no of classes in th dataset. In this case it is 10\n",
        "\n",
        "learning_rate = 0.1\n",
        "num_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDrf-XCcGWcS",
        "colab_type": "text"
      },
      "source": [
        "## Load the MNIST data. \n",
        "For convenience we have already downloaded the MNIST dataset and saved in the '../../data' folder. So, the argument download is set to 'False'. We then whiten the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7aGTYDGWcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_dataset = datasets.MNIST('./data', train=True, download=True,\n",
        "                                                          transform=transforms.Compose([\n",
        "                                                              transforms.ToTensor(),\n",
        "                                                              transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU1s2HHLGWcV",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid activation function and its derivative\n",
        "\n",
        "$\\sigma(x)=\\frac{1}{1+e^{-x}}$\n",
        "\n",
        "$\\sigma^{'}(x) = \\sigma(x)(1-\\sigma(x))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnbfgh3VGWcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1/torch.exp(x.mul(-1)).add(1)\n",
        "    \n",
        "\n",
        "def sigmoid_diff(x):\n",
        "    return torch.mul(sigmoid(x), sigmoid(x).mul(-1).add(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd6vZPYHqDMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper Class and Functions\n",
        "class Metrics:\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val*n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "        \n",
        "        \n",
        "def compute_accuracy(output, target, topk=(1,)):\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    result = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        result.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return result\n",
        "\n",
        "\n",
        "def one_hot(data, max_value):\n",
        "    ones = torch.sparse.torch.eye(max_value)\n",
        "    return ones.index_select(0, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HQ7T2VLGWcX",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the weight matrices with some random values\n",
        "\n",
        "$W_1 \\in \\mathbb{R}^{784 x 100}$\n",
        "\n",
        "$W_2 \\in \\mathbb{R}^{100 x 10}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O4qiKXZGWcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initiliaze the weights\n",
        "W_1 = torch.randn(input_dim, num_of_hidden_nodes).type(torch.FloatTensor) # Weights between input and hidden layer\n",
        "W_2 = torch.randn(num_of_hidden_nodes, output_dim).type(torch.FloatTensor) # Weights between hidden layer and output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIXwr4ldGWca",
        "colab_type": "text"
      },
      "source": [
        "## The training loop with manual backpropagation\n",
        "\n",
        "In each epoch, we will have several batches of data. We take each of the batches and do the forward pass. Then based on the error we back-propagate.\n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Junita_Mohamad-Saleh/publication/257071174/figure/fig3/AS:297526545666050@1447947264431/A-schematic-diagram-of-a-Multi-Layer-Perceptron-MLP-neural-network.png \"MLP with 3-layers\")\n",
        "\n",
        "\n",
        "Assume, batch_size = 1, matrix multiplication $*$ and element-wise multiplication $.$\n",
        "\n",
        "### Mean-Squared Loss Function:\n",
        "\n",
        "$L = 0.5*(output - true\\_output)^2$\n",
        "\n",
        "### Forward Pass:\n",
        "\n",
        "$Z = \\sigma(W_1^{T}X)$           [$\\mathbb{R}^{1 x 100}$]\n",
        "\n",
        "$output = \\sigma(W_2^{T}Z)$       [$\\mathbb{R}^{1 x 10}$]\n",
        "\n",
        "### Backward Pass:\n",
        "\n",
        "Derivative of loss: $diff = (output - true\\_output)$   [$\\mathbb{R}^{1 x 10}$]\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_2} = Z^{T}*(diff.\\sigma^{'}(output))$    [$\\mathbb{R}^{100 x 10}$]\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_1} = X^{T} *((diff.\\sigma^{'}(output))*W_2^{T}).\\sigma^{'}(Z)$ [$\\mathbb{R}^{784 x 100}$]\n",
        "\n",
        "### Parameter Update:\n",
        "\n",
        "$W_1 = W_1 - \\eta \\frac{\\partial L}{\\partial W_1}$\n",
        "\n",
        "$W_2 = W_2 - \\eta \\frac{\\partial L}{\\partial W_2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjoozdwFGWca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(0, num_epochs):\n",
        "    loss = 0\n",
        "    accuracy_metric = Metrics()\n",
        "\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        # Forward Pass\n",
        "        x_batch = x_batch.view(-1, 784)\n",
        "        hidden_state_output = sigmoid(torch.mm(x_batch, W_1))\n",
        "        output = sigmoid(torch.mm(hidden_state_output, W_2))\n",
        "\n",
        "        # Convert the labels to one hot encoded format\n",
        "        y_batch_onehot = one_hot(y_batch, 10)\n",
        "\n",
        "        # Loss (Mean-Squared error)\n",
        "        loss = (output - y_batch_onehot).pow(2).sum() * 0.5\n",
        "\n",
        "        # Backward Pass (Back-Propagation)\n",
        "        # Derivative of MSE Loss\n",
        "        diff = (output - y_batch_onehot)\n",
        "\n",
        "        grad_w2 = torch.mm(hidden_state_output.t(), torch.mul(diff, sigmoid_diff(output)))  # 100 x 10 dimensional\n",
        "        grad_w1 = torch.mm(x_batch.t(), torch.mul(torch.mm(torch.mul(diff, sigmoid_diff(output)), W_2.t())\n",
        "                                                  , sigmoid_diff(hidden_state_output)))  # 784 x 100\n",
        "\n",
        "        # Perform gradient descent\n",
        "        W_1 -= learning_rate * grad_w1\n",
        "        W_2 -= learning_rate * grad_w2\n",
        "\n",
        "        accuracy = compute_accuracy(output, y_batch)\n",
        "        accuracy_metric.update(accuracy[0])\n",
        "\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(\"Epoch: {0} \\t|\\t loss: {1} \\t|\\t accuracy: {2}\".format(epoch, loss, accuracy_metric.avg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SxCNCKvGWcd",
        "colab_type": "text"
      },
      "source": [
        "## Using in-built Autograd function\n",
        "\n",
        "loss.backward():  calculates the gradients of the loss function w.r.t all the parameters in the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89FTCCh-GWcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "W_1_ag = torch.randn(input_dim, num_of_hidden_nodes, requires_grad=True)\n",
        "W_2_ag = torch.randn(num_of_hidden_nodes, output_dim, requires_grad=True)\n",
        "\n",
        "for epoch in range(0, num_epochs):\n",
        "\n",
        "    correct = 0\n",
        "    accuracy_metric = Metrics()\n",
        "\n",
        "    for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "\n",
        "        x_batch = x_batch.view(-1,784)\n",
        "\n",
        "        # Forward Pass\n",
        "        hidden_state_output = torch.sigmoid(torch.mm(x_batch, W_1_ag))\n",
        "        output = torch.sigmoid(torch.mm(hidden_state_output, W_2_ag))\n",
        "\n",
        "        # Convert the labels to one hot encoded format\n",
        "        y_batch_onehot = one_hot(y_batch, 10)\n",
        "\n",
        "        # Loss (Mean-Squared error)\n",
        "        loss = (output - y_batch_onehot).pow(2).sum().mul(0.5)\n",
        "        loss.backward()\n",
        "\n",
        "        W_1_ag.data -= learning_rate * W_1_ag.grad.data\n",
        "        W_2_ag.data -= learning_rate * W_2_ag.grad.data\n",
        "\n",
        "        # Manually zero the gradients before running the backward pass\n",
        "        W_1_ag.grad.data.zero_()\n",
        "        W_2_ag.grad.data.zero_()\n",
        "\n",
        "        accuracy = compute_accuracy(output, y_batch)\n",
        "        accuracy_metric.update(accuracy[0])\n",
        "\n",
        "        if batch_idx % 200 == 0:\n",
        "            print(\"Epoch: {0} \\t|\\t loss: {1} \\t|\\t accuracy: {2}\".format(epoch, loss ,accuracy_metric.avg))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg0GypGzOXF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}